{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv use panda\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r3/tyn9zxnn1nz5j5rq7xdz45br0000gt/T/ipykernel_29335/25287263.py:96: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  verification_combined.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the two verification datasets again to ensure correct handling\n",
    "verification_v1 = pd.read_csv('new data/CLD data - Verification_data V1.csv')\n",
    "verification_v2 =pd.read_csv('new data/CLD data - Verification_data V2.csv')\n",
    "\n",
    "# Add version columns\n",
    "verification_v1['version'] = 'V1'\n",
    "verification_v2['version'] = 'V2'\n",
    "\n",
    "# Concatenate DataFrames\n",
    "verification_combined = pd.concat([verification_v1, verification_v2], ignore_index=True)\n",
    "\n",
    "\n",
    "verification_combined['source_color'] = verification_combined['source_color'].fillna('0').astype('int').astype(str)\n",
    "verification_combined['target_color'] = verification_combined['target_color'].fillna('0').astype('int').astype(str)\n",
    "verification_combined['link_color'] = verification_combined['link_color'].fillna('0').astype('int').astype(str)\n",
    "# Process participant_type column\n",
    "verification_combined['participant_type'] = (\n",
    "    verification_combined['participant_type'].str.strip().str.lower().str.replace(' ', '_')\n",
    ")\n",
    "verification_combined['group'] = verification_combined['participant_type'] + '_' + verification_combined['participant_number'].astype(str)\n",
    "\n",
    "# Define column mapping\n",
    "column_mapping = {\n",
    "    'cause': 'source',\n",
    "    'effect': 'target',\n",
    "    'Master ID': 'source_master_id',\n",
    "    'Node ID': 'source_node_id',\n",
    "    'Master ID.1': 'target_master_id',\n",
    "    'Node ID.1': 'target_node_id',\n",
    "    'polarity': 'polarity',\n",
    "    'participant_number': 'participant_number',\n",
    "    'participant_type': 'participant_type',\n",
    "    'source_color': 'source_color',\n",
    "    'target_color': 'target_color',\n",
    "    'Comment': 'comment'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "verification_combined.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Function to categorize comments\n",
    "def map_comment_to_type(row):\n",
    "    if row['version'] == 'V1':\n",
    "        return 'O'\n",
    "    elif pd.isna(row['comment']):\n",
    "        return 'K'\n",
    "    \n",
    "    short_comment = ''.join(w.capitalize()[0] for w in row['comment'].split())\n",
    "    \n",
    "    return short_comment\n",
    "\n",
    "# Add types column\n",
    "verification_combined['types'] = verification_combined.apply(map_comment_to_type, axis=1)\n",
    "\n",
    "\n",
    "# Load Node_Mapping dataset\n",
    "node_mapping = pd.read_csv('new data/CLD data - Node Mapping.csv')\n",
    "\n",
    "# Prepare index mapping for source and target nodes\n",
    "\n",
    "# Create node_v1_index_map\n",
    "node_v1_index_map = node_mapping.set_index('Node ID 1')['Index'].to_dict()\n",
    "\n",
    "# Create node_v2_index_map\n",
    "node_v2_index_map = node_mapping.set_index('Node ID 2')['Index'].to_dict()\n",
    "\n",
    "node_index_map = {**node_v1_index_map, **node_v2_index_map}\n",
    "\n",
    "# Define a function to filter out NaN keys and values\n",
    "def filter_nan_from_dict(d):\n",
    "    return {k: v for k, v in d.items() if pd.notna(k) and pd.notna(v)}\n",
    "\n",
    "# Filter node_index_map\n",
    "node_index_map = filter_nan_from_dict(node_index_map)\n",
    "\n",
    "node_final = pd.read_csv('new data/CLD data - Node Final.csv')\n",
    "node_final_master_map = node_final.set_index('Node ID')['Master ID'].to_dict()\n",
    "\n",
    "\n",
    "merged_node_index_map = node_index_map.copy()  # Start with node_index_map\n",
    "for key, value in node_final_master_map.items():\n",
    "    if key not in node_index_map:\n",
    "        merged_node_index_map[key] = value\n",
    "\n",
    "\n",
    "# Function to map node IDs to indices\n",
    "def map_node_id_to_index(node_id, node_map):\n",
    "    return str(merged_node_index_map.get(node_id))\n",
    "\n",
    "# Apply mapping for source_node_id and target_node_id\n",
    "verification_combined['source_map_id'] = verification_combined['source_node_id'].apply(map_node_id_to_index, node_map=node_index_map)\n",
    "verification_combined['target_map_id'] = verification_combined['target_node_id'].apply(map_node_id_to_index, node_map=node_index_map)\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "verification_combined.fillna('', inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "verification_combined.head()\n",
    "\n",
    "verification_combined.to_csv('new_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_from_df(df,node_history_map):\n",
    "\n",
    "    nodes = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for node_type in [\"source\", \"target\"]:\n",
    "            node_id_key = f\"{node_type}_node_id\"\n",
    "            node_map_key = f\"{node_type}_map_id\"\n",
    "            node_color_key = f\"{node_type}_color\"\n",
    "\n",
    "            node_data = {\n",
    "                \"data\": {\n",
    "                    \"id\": row[node_map_key],\n",
    "                    \"key\": row[node_id_key],\n",
    "                    \"label\": row[node_type],\n",
    "                    \"participant_type\": row[\"participant_type\"],\n",
    "                    \"color\": row[node_color_key],\n",
    "                    \"history\": node_history_map.get(row[node_map_key], []),\n",
    "                },\n",
    "                \"classes\": \"\",\n",
    "            }\n",
    "            nodes.append(node_data)\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def get_edges_from_df(df):\n",
    "    edges = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"polarity\"] == \"positive\":\n",
    "            value = 1\n",
    "            sign = \"+\"\n",
    "        elif row[\"polarity\"] == \"negative\":\n",
    "            value = -1\n",
    "            sign = \"-\"\n",
    "        else:\n",
    "            value = 0  # or some other default value\n",
    "            sign = row[\"polarity\"]  # or any other default handling\n",
    "\n",
    "        edge_data = {\n",
    "            \"data\": {\n",
    "                # \"id\": f\"{row['source_node_id']}-{row['target_node_id']}\",\n",
    "                \"id\": f\"{row['source_map_id']}-{row['target_map_id']}\",\n",
    "                \"label\": sign,\n",
    "                \"value\": value,\n",
    "                \"source\": row[\"source_node_id\"],\n",
    "                \"target\": row[\"target_node_id\"],\n",
    "                \"source\": row[\"source_map_id\"],\n",
    "                \"target\": row[\"target_map_id\"],\n",
    "                \"polarity\": row[\"polarity\"],\n",
    "                \"participant_type\": row[\"participant_type\"],\n",
    "                \"group\": row[\"group\"],\n",
    "                \"color\": row[\"link_color\"],\n",
    "            }\n",
    "        }\n",
    "        edges.append(edge_data)\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "# Split the dataset based on 'group' column\n",
    "dataset_group_by_group = {\n",
    "    group: data for group, data in verification_combined.groupby(\"group\")\n",
    "}\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    # Perform desired operations on the dataframe\n",
    "    node_history_map = {}\n",
    "    df_v1 = df[df['version'] == \"V1\"]\n",
    "\n",
    "    for _, row in df_v1.iterrows():\n",
    "        for node_type in [\"source\", \"target\"]:\n",
    "            node_id_key = f\"{node_type}_node_id\"\n",
    "            node_master_key = f\"{node_type}_master_id\"\n",
    "            node_map_key = f\"{node_type}_map_id\"\n",
    "            node_history_entry = {\n",
    "                \"id\": row[node_map_key],\n",
    "                \"key\": row[node_master_key],\n",
    "                \"label\": row[node_type],\n",
    "                \"participant_type\": row['participant_type'],\n",
    "                \"group\": row['group'],\n",
    "                \"history\": []\n",
    "            }\n",
    "     \n",
    "            if row[node_type] in node_history_map:\n",
    "                if node_history_entry not in node_history_map[row[node_type]]:\n",
    "                    node_history_map[row[node_map_key]].append(node_history_entry)\n",
    "            else:\n",
    "                node_history_map[row[node_map_key]] = [node_history_entry]\n",
    "\n",
    "    processed_data = {\n",
    "        \"final\" if version == 'V2' else version: {\n",
    "            \"nodes\": get_nodes_from_df(data, node_history_map),\n",
    "            \"edges\": get_edges_from_df(data),\n",
    "            # 'raw_data': data.to_dict(orient='records')\n",
    "        }\n",
    "        for version, data in df.groupby(\"version\")\n",
    "    }\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Display each group as separate datasets with processed data\n",
    "json_data = {\n",
    "    group: process_dataframe(df) for group, df in dataset_group_by_group.items()\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"new_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
